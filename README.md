# NLP

The main aim of this project is :
To implement Transformers with attention mechanism to translate German to English. 
To implement Sequence to Sequence (LSTM) and Sequence to Sequence with Attention (LSTM) to compare their performances with Transformers with attention.
To compare the performances of the models with different learning rates and optimizers
To propose the efficient combination. 
To explore different Tokenization methods and compare their performances.

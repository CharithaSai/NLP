# NLP


NeuralMachineTranslation.ipynb file has the implementation of all the three models.			
Performance of Tokenizer.py file has the implementation of different tokenization methods.			
Gita.txt file has the data which is used for tokenization.

The main aim of this project is :
To implement Transformers with attention mechanism to translate German to English. 
To implement Sequence to Sequence (LSTM) and Sequence to Sequence with Attention (LSTM) to compare their performances with Transformers with attention.
To compare the performances of the models with different learning rates and optimizers
To propose the efficient combination. 
To explore different Tokenization methods and compare their performances.
